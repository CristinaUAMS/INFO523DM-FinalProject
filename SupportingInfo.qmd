---
title: "From Trees to Chains: Implementing Bayesian Additive Regression with MCMC"
subtitle: "INFO 523 - Summer 2024 - Final Project"
description: "Presentation on Bayesian Additive Regression Trees with MCMC"
author: 
- name: Cristina Lafuente
  affiliation: Department of Electrical and Computer Engineering, University of Arizona
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
---

# Abstract

Bayesian Additive Regression Trees (BART) with Markov Chain Monte Carlo (MCMC) is a powerful approach for predictive modeling. This paper and presentation explore BART algorithm focusing on the additive tree structure and the role of MCMC in efficiently estimating model parameters. Contained within is a worked example using a publicly available dataset of earned tips demonstrating how BART with MCMC captures the complex relationships and provides uncertainty estimates. Further, there is discussion of other real-world applications of this approach including (but not limited to): healthcare risk prediction, environmental monitoring, and market forecasting.

# Introduction

## Background

Bayesian Additive Regression Trees with Markov Chain Monte Carlo creates a robust approach for predictive modeling. It can model complex, non-linear relationships without specific modeling of interactions. Bayesian approach provides uncertainty estimates which help give context and understanding to the model, especially with respect to reliability. By averaging over many trees, it is less likely to be over-fitted than if a single, large tree were used. BART is a non-parametric Bayesian method for regression and classification. It can model complex relationships by combining multiple simple decision trees to predict an outcome. The additive refers to the fact that the overall prediction is the sum of the contributions from each tree. It uses Bayesian inference to estimate the posterior distribution of the model's parameters providing point estimates and measures of uncertainty in predictions. MCMC methods generate samples from the posterior distribution of the parameters by constructing a Markov chain with the desired distribution as the equilibrium distribution. Using the combined method, the overly complex trees are penalized to prevent over-fitting. During sampling, the trees and their structure are updated based on data and prior distribution. Once sampling is complete, the final prediction for a new observation is developed by averaging the predictions from all the sampled trees.

## Details

#### Equation Basics

Given the standard polynomial regression equation: <br/><br/> $E [Y] = \beta_0 +\beta_1 B_1(X_1)+\beta_2 B_2(X_2) + \cdot \cdot \cdot + \beta_m B_m(X_m)$

Assuming that the ***B~i~*** basis functions in a standard polynomial regression equation are decision trees, the equation becomes: <br/><br/>

$E[Y] = \phi (\sum_{j=0}^m g_j(X_i T_j, M_j), \theta)$<br/> where: <br/> each ***g~j~*** is a tree of the form ***g(X~i~ T~j~, M~j~)*** <br/> ***T~j~*** represents the structure of the binary tree (the set of internal nodes and their decision rules and a set of terminal nodes) <br/> $M_j = \{\mu_{1,j}, \mu_{2,j}, \cdot\cdot\cdot, \mu_{b,j}\}$ represents the values at the ***b~j~*** terminal nodes <br/> $\phi$ represents an arbitrary probability distribution that will be used as the likelihood in the model <br/> $\theta$ other parameters from $\phi$ not modeled as a sum of trees <br/>

Some examples of probability distributions might be a Gaussian, a Poisson distribution, or a t-distribution. <br/>

#### Priors

To specify the BART model, priors need to be specified. These priors help shape the model's structure, control the complexity and guide the learning process. Those prior specifications can be: <br/>

-   Tree Structure and Complexity: Limiting tree depth and size. <br/>
-   Terminal Node Values: Specifying the distribution for leaf values (i.e. normal distribution) <br/>
-   Splits and Growth: Controlling the likelihood and nature of tree splits and number of trees within the ensemble. <br/>
-   Weights and Hyperparameters: Setting priors to balance flexibility and regularization <br/>

**Here we will use:** <br/> **Independence:** In order to simplify the specification of the prior, assume that the structure of the tree ***T~j~*** and the leaf values ***M~j~*** are independent. <br/> **Tree Structure:** <br/> 1 - Probability that a node at depth $d = (0, 1, 2, ...)$ is non terminal is given by $\alpha^d$ where $\alpha$ is recommended to be $\in [0, 0.5)$. <br/> 2 - Distribution over the splitting variable: which covariate is included in the tree ***X~i~*** most commonly uniform over available covariates. <br/> 3 - Distribution over the splitting variable: once the splitting variable is chosen, which value is used to make a decision. see figure below for illustration of splitting rule

![Source: https://bayesiancomputationbook.com/markdown/chp_07.html](images/BinaryTreeWithPartitionSpace.png) Leaf Values $\mu_{ij}$ and Number of Trees ***m***: Best results are acheived from setting a specific value of m (usually between 10 and 200) and cross validating. <br/>

#### MCMC Sampling Process

Markov Chain Monte Carlo is used to sample from the posterior distribution of the parameters. It generates samples of the regression trees' structures - arrangement of splits and terminal nodes - exploring possible tree configurations. <br/> Each tree's terminal node values are sampled from their posterior distributions. The node values usually folow a normal distribution with mean and variance specified by the model.<br/>

**Algorithm:** <br/> Start with initial configuration of trees - either randomly generated or based on some specified method. <br/> **Iterate:** **Update Structures.** Changes to the current set of trees are proposed - adding or deleting trees or modifying the structure of existing trees. The changes are accepted or rejected based on probability given data and the prior distribution. **Update Terminal Node Values** For each tree update the values at the terminal nodes. Sample from the posterior distribution of the values given the current tree structure and data. **Acceptance Criteria** Use a set criterion to decide whether to accept proposed changes.<br/> **Convergence:** Continue sampling until the Markov chain converges to the stationary distribution which approximates the posterior distribution of the model parameters.<br/><br/> Once MCMC sampling is complete, predictions for new data are obtained by averaging predictions from all sampled trees. Each tree contributes to the final prediction based on its sampled weight.<br/> The variablity in predictions across sampled trees reflects uncertainty in the model. <br/>

#### Purpose of the Model

The purpose of the Bayesian Additive Regression Trees with Markov Chain Monte Carlo is to provide a robust and flexible framework for predictive modeling and inference using both the Bayesian principles and regression tree methods. This method models complex relationships and quantifies uncertainty. It prevents overfitting while delivering accurate predictions for a variety of real-world applications. It is the best of the Bayesian inference and tree-based models. 


## Relation of this topic to course

BART with MCMC is related to the following course topics:<br/>

-   Regression<br/>

-   Overfitting<br/>

-   Regression Trees<br/>

-   Bayesian Methods<br/>

-   Data manipulation<br/>

-   Ability to handle non-linearity and interactions<br/>

## Real-world applications

BART with MCMC is useful in a number of different real-life scenarios. The common aspect of those is that the data exhibits non-linear relationships and interactions between the variables, there is need for uncertainty estimates, high accuracy is preferable.<br/><br/>
Some specific examples include:<br>
 -  **Finance: ** From credit risk assessment to stock price prediction, financial analysis is complex and non-linear and requires in depth analysis which this method can provide.<br/>
 -  **Healthcare: ** Disease risk modeling based on risk factors where there are innumerable interactions to patient outcome predictions are complex and difficult to model with some of the more simple models.<br/>
 -  **Marketing: ** Customer behaviors can be complicated and this model is more able than most to examine the many different aspects.<br/>
 - **Environmental Science: ** From climate modeling to ecological modeling, this method can help scientists examine different variables, their impacts and patterns. This can help with conservation planning and assessing environmental impact. <br/>
 -  **Engineering: ** Detection of anomalies or faults based on sensor data before failure has occurred or in quality control by identifying factors which affect defects. Uncertainty estimates can help specificly identify quality control issues.<br/>
 -  **Epidemiology: ** BART can model everything from disease spread to intervention effectiveness and give uncertainty estimates which are important for understanding and predicting points of possible outbreaks.<br/> 
 - **Real Estate: ** BART can be used to predict property values based on location, size, amenities as well as markets. It can also be used to forecast future prices through market trend analysis.<br/>


```{r}
#| label: load-packages
#| include: false

options(java.parameters = "-Xmx5g")
if (!require("pacman")) 
  install.packages("pacman")

pacman::p_load(bartMachine,
               bartMan,
               caTools,
               dplyr,
               ggplot2,
               ggthemes,
               here,
               MASS,
               partykit,
               pdp,
               rJava,
               rpart,
               rpart.plot,
               tidyverse,
               treemap)

```

## The data

```{r}
#| label: load-data
#| include: false
data("Boston")
boston_data <- Boston

```

The Boston Housing dataset consists of various features related to housing, including:<br/>

 -  crim: Per capita crime rate by town<br/>
 -  zn: Proportion of residential land zoned for lots over 25,000 sq. ft.<br/>
 -  indus: Proportion of non-retail business acres per town<br/>
 -  chas: Charles River dummy variable (1 if tract bounds river; 0 otherwise)<br/>
 -  nox: Nitric oxides concentration (parts per 10 million)<br/>
 -  rm: Average number of rooms per dwelling<br/>
 -  age: Proportion of owner-occupied units built prior to 1940<br/>
 -  dis: Weighted distances to five Boston employment centers<br/>
 -  rad: Index of accessibility to radial highways<br/>
 -  tax: Full-value property tax rate per $10,000<br/>
 -  ptratio: Pupil-teacher ratio by town<br/>
 -  black: 1000(Bk - 0.63)^2 where Bk is the proportion of Black residents by town<br/>
 -  lstat: Percentage of lower status of the population<br/>
 -  medv: Median value of owner-occupied homes in $1000s (target variable)<br/><br/>
 

The data looks like: 

```{r}
#| label: data
#| include: false


glimpse(head(boston_data))
```

# Worked Example with given Data
```{r}
#|label: worked-example

set.seed(1123)

## Split data into training and testing 
split <- sample.split(boston_data$medv, SplitRatio = 0.7)
train_data <- subset(boston_data, split == TRUE)
test_data <- subset(boston_data, split == FALSE)

X_train <- train_data[, -14]
y_train <- train_data$medv

X_test <- test_data[, -14]
y_test <- test_data$medv

bart_model <- bartMachine(X_train, y_train)
y_pred <- predict(bart_model, X_test)

mse <- mean((y_test - y_pred)^2)
print(paste('Mean Squared Error: ', mse))

importance <- investigate_var_importance(bart_model)

#node_data <- extract_raw_node_data(bart_model)

#treeData <- extractTreeData(bart_model, tipsData)
prediction <- predict(bart_model, X_train)
ggplot(data.frame(Actual = y_train, Predicted = prediction),
       aes(x = Actual, y = Predicted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  xlab("Actual Values") +
  ylab("Predicted Values") +
  ggtitle("Actual V Predicted Values")

posterior <- bart_machine_get_posterior(bart_model, X_train)
#print(posterior$y_hat)

hist(posterior$y_hat_posterior_samples, 
     main = "Posterior Distribution of Predictions",
     xlab = "Value", col = "darkslateblue")

error_assumptions <- check_bart_error_assumptions(bart_model, 
                                                  hetero_plot = "yhats")

cov_importance_test(bart_model)

prop_incl <- get_var_props_over_chain(bart_model)
print(prop_incl)

posterior_samples <- posterior$y_hat_posterior_samples

predictions <- as.matrix(predict(bart_model, X_train))

predicted_means <- rowMeans(posterior_samples)
predicted_lows <- apply(posterior_samples, 1,
                        function(x) quantile(x, probs = 0.025))
predicted_highs <- apply(posterior_samples, 1, 
                         function(x) quantile(x, probs = 0.975))

prediction_intervals <- data.frame(
  Mean = predicted_means,
  Lower = predicted_lows,
  Upper = predicted_highs
)

plot_data <- data.frame(
  observed = y_train,
  predicted = predicted_means,
  lower = predicted_lows,
  upper = predicted_highs
)


ggplot(plot_data, aes(x = predicted, y = observed)) +
  geom_point(color = "royalblue4", size = 2) +
  geom_errorbar(aes(ymin = lower, ymax = upper), 
                width = .2, color = "lightslateblue") +
  geom_abline(intercept = 0, slope = 1, color = "magenta3") +
  scale_y_continuous() +
  labs(
    title = "Predictions with Uncertainty Intervals",
    x = "Predicted", 
    y = "Observed (log_10)") +
  theme(
    text = element_text(color = "white"),
    axis.text = element_text(color = "white"),
    plot.background = element_rect(fill = "steelblue4"),
    panel.background = element_rect(fill = "cornsilk", color = "grey")

  )
```

# Conclusions, Recommendations, Future Work

# References

Bayesian Modeling and Computation in Python. (2022). Bayesiancomputationbook.com. https://bayesiancomputationbook.com , Hugh A. Chipman, Edward I. George, and Robert E. McCulloch. <br/>

Veronika Ročková and Enakshi Saha. On theory for bart. In The 22nd International Conference on Artificial Intelligence and Statistics, 2839–2848. PMLR, 2019. <br/>
