---
title: "From Trees to Chains: Implementing Bayesian Additive Regression with MCMC"
subtitle: "INFO 523 - Summer 2024 - Final Project"
description: "Presentation on Bayesian Additive Regression Trees with MCMC"
author: 
- name: Cristina Lafuente
  affiliation: Department of Electrical and Computer Engineering, University of Arizona
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
---

# Abstract

Bayesian Additive Regression Trees (BART) with Markov Chain Monte Carlo (MCMC) is a powerful approach for predictive modeling. This paper and presentation explore BART algorithm focusing on the additive tree structure and the role of MCMC in efficiently estimating model parameters. Contained within is a worked example using a publicly available dataset of earned tips demonstrating how BART with MCMC captures the complex relationships and provides uncertainty estimates. Further, there is discussion of other real-world applications of this approach including (but not limited to): healthcare risk prediction, environmental monitoring, and market forecasting.

# Introduction

## Background

Bayesian Additive Regression Trees with Markov Chain Monte Carlo creates a robust approach for predictive modeling. It can model complex, non-linear relationships without specific modeling of interactions. Bayesian approach provides uncertainty estimates which help give context and understanding to the model, especially with respect to reliability. By averaging over many trees, it is less likely to be over-fitted than if a single, large tree were used. BART is a non-parametric Bayesian method for regression and classification. It can model complex relationships by combining multiple simple decision trees to predict an outcome. The additive refers to the fact that the overall prediction is the sum of the contributions from each tree. It uses Bayesian inference to estimate the posterior distribution of the model's parameters providing point estimates and measures of uncertainty in predictions. MCMC methods generate samples from the posterior distribution of the parameters by constructing a Markov chain with the desired distribution as the equilibrium distribution. Using the combined method, the overly complex trees are penalized to prevent over-fitting. During sampling, the trees and their structure are updated based on data and prior distribution. Once sampling is complete, the final prediction for a new observation is developed by averaging the predictions from all the sampled trees.

## Details

#### Equation Basics

Given the standard polynomial regression equation: <br/><br/> $E [Y] = \beta_0 +\beta_1 B_1(X_1)+\beta_2 B_2(X_2) + \cdot \cdot \cdot + \beta_m B_m(X_m)$

Assuming that the ***B~i~*** basis functions in a standard polynomial regression equation are decision trees, the equation becomes: <br/><br/>

$E[Y] = \phi (\sum_{j=0}^m g_j(X_i T_j, M_j), \theta)$<br/> where: <br/> each ***g~j~*** is a tree of the form ***g(X~i~ T~j~, M~j~)*** <br/> ***T~j~*** represents the structure of the binary tree (the set of internal nodes and their decision rules and a set of terminal nodes) <br/> $M_j = \{\mu_{1,j}, \mu_{2,j}, \cdot\cdot\cdot, \mu_{b,j}\}$ represents the values at the ***b~j~*** terminal nodes <br/> $\phi$ represents an arbitrary probability distribution that will be used as the likelihood in the model <br/> $\theta$ other parameters from $\phi$ not modeled as a sum of trees <br/>

Some examples of probability distributions might be a Gaussian, a Poisson distribution, or a t-distribution. <br/>

#### Priors

To specify the BART model, priors need to be specified. These priors help shape the model's structure, control the complexity and guide the learning process. Those prior specifications can be: <br/>

-   Tree Structure and Complexity: Limiting tree depth and size. <br/>
-   Terminal Node Values: Specifying the distribution for leaf values (i.e. normal distribution) <br/>
-   Splits and Growth: Controlling the likelihood and nature of tree splits and number of trees within the ensemble. <br/>
-   Weights and Hyperparameters: Setting priors to balance flexibility and regularization <br/>

**Here we will use:** <br/> **Independence:** In order to simplify the specification of the prior, assume that the structure of the tree ***T~j~*** and the leaf values ***M~j~*** are independent. <br/> **Tree Structure:** <br/> 1 - Probability that a node at depth $d = (0, 1, 2, ...)$ is non terminal is given by $\alpha^d$ where $\alpha$ is recommended to be $\in [0, 0.5)$. <br/> 2 - Distribution over the splitting variable: which covariate is included in the tree ***X~i~*** most commonly uniform over available covariates. <br/> 3 - Distribution over the splitting variable: once the splitting variable is chosen, which value is used to make a decision. see figure below for illustration of splitting rule

![Source: https://bayesiancomputationbook.com/markdown/chp_07.html](images/BinaryTreeWithPartitionSpace.png) Leaf Values $\mu_{ij}$ and Number of Trees ***m***: Best results are acheived from setting a specific value of m (usually between 10 and 200) and cross validating. <br/>

#### MCMC Sampling Process

Markov Chain Monte Carlo is used to sample from the posterior distribution of the parameters. It generates samples of the regression trees' structures - arrangement of splits and terminal nodes - exploring possible tree configurations. <br/> Each tree's terminal node values are sampled from their posterior distributions. The node values usually folow a normal distribution with mean and variance specified by the model.<br/>

**Algorithm:** <br/> Start with initial configuration of trees - either randomly generated or based on some specified method. <br/> **Iterate:** **Update Structures.** Changes to the current set of trees are proposed - adding or deleting trees or modifying the structure of existing trees. The changes are accepted or rejected based on probability given data and the prior distribution. **Update Terminal Node Values** For each tree update the values at the terminal nodes. Sample from the posterior distribution of the values given the current tree structure and data. **Acceptance Criteria** Use a set criterion to decide whether to accept proposed changes.<br/> **Convergence:** Continue sampling until the Markov chain converges to the stationary distribution which approximates the posterior distribution of the model parameters.<br/><br/> Once MCMC sampling is complete, predictions for new data are obtained by averaging predictions from all sampled trees. Each tree contributes to the final prediction based on its sampled weight.<br/> The variablity in predictions across sampled trees reflects uncertainty in the model. <br/>

#### Purpose of the Model

The purpose of the Bayesian Additive Regression Trees with Markov Chain Monte Carlo is to provide a robust and flexible framework for predictive modeling and inference using both the Bayesian principles and regression tree methods. This method models complex relationships and quantifies uncertainty. It prevents overfitting while delivering accurate predictions for a variety of real-world applications. It is the best of the Bayesian inference and tree-based models. 


## Relation of this topic to course

BART with MCMC is related to the following course topics:<br/>

-   Regression<br/>

-   Overfitting<br/>

-   Regression Trees<br/>

-   Bayesian Methods<br/>

-   Data manipulation<br/>

-   Ability to handle non-linearity and interactions<br/>

## Real-world applications

```{r}
#| label: load-packages
#| include: false


if (!require("pacman")) 
  install.packages("pacman")

pacman::p_load(dplyr,
               ggthemes,
               here,
               tidyverse)

```

## The data

```{r}
#| label: load-data
#| include: false

tips <- read.csv(here("data/tips.csv"))

```

This data is the tip data collected by a waiter over a period of a couple months. The variables include: <br/> \* tip in dollars<br/> \* bill in dollars<br/> \* sex of the bill payer<br/> \* whether there were smokers in the party<br/> \* day of the week<br/> \* time of day<br/> \* size of the party<br/><br/> Data found at https://rdrr.io/cran/reshape2/man/tips.html <br/> Reference given: Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing

```{r}
#| label: manip-data
#| include: false

tips_wPerc <- tips |>
  mutate(
    tip_percent = round((tip / total_bill) * 100, 2)
  )

```

# Worked Example with given Data

# Conclusions, Recommendations, Future Work

# References

Bayesian Modeling and Computation in Python. (2022). Bayesiancomputationbook.com. https://bayesiancomputationbook.com , Hugh A. Chipman, Edward I. George, and Robert E. McCulloch. <br/>

Veronika Ročková and Enakshi Saha. On theory for bart. In The 22nd International Conference on Artificial Intelligence and Statistics, 2839–2848. PMLR, 2019. <br/>
