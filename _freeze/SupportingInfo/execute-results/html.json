{
  "hash": "f80b14c2616b69c557c30d794251a35c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"From Trees to Chains: Implementing Bayesian Additive Regression with MCMC\"\nsubtitle: \"INFO 523 - Summer 2024 - Final Project\"\ndescription: \"Presentation on Bayesian Additive Regression Trees with MCMC\"\nauthor: \n- name: Cristina Lafuente\n  affiliation: Department of Electrical and Computer Engineering, University of Arizona\nformat:\n   html:\n    code-tools: true\n    code-overflow: wrap\n    embed-resources: true\neditor: visual\nexecute:\n  warning: false\n  echo: false\n---\n\n\n# Abstract\n\nBayesian Additive Regression Trees (BART) with Markov Chain Monte Carlo (MCMC) is a powerful approach for predictive modeling. This paper and presentation explore BART algorithm focusing on the additive tree structure and the role of MCMC in efficiently estimating model parameters. Contained within is a worked example using a publicly available dataset Boston housing data demonstrating how BART with MCMC captures the complex relationships and provides uncertainty estimates. Further, there is discussion of other real-world applications of this approach including (but not limited to): healthcare risk prediction, environmental monitoring, and market forecasting.\n\n# Introduction\n\n## Background\n\nBayesian Additive Regression Trees with Markov Chain Monte Carlo creates a robust approach for predictive modeling. It can model complex, non-linear relationships without specific modeling of interactions. Bayesian approach provides uncertainty estimates which help give context and understanding to the model, especially with respect to reliability. By averaging over many trees, it is less likely to be over-fitted than if a single, large tree were used. BART is a non-parametric Bayesian method for regression and classification. It can model complex relationships by combining multiple simple decision trees to predict an outcome. The additive refers to the fact that the overall prediction is the sum of the contributions from each tree. It uses Bayesian inference to estimate the posterior distribution of the model's parameters providing point estimates and measures of uncertainty in predictions. MCMC methods generate samples from the posterior distribution of the parameters by constructing a Markov chain with the desired distribution as the equilibrium distribution. Using the combined method, the overly complex trees are penalized to prevent over-fitting. During sampling, the trees and their structure are updated based on data and prior distribution. Once sampling is complete, the final prediction for a new observation is developed by averaging the predictions from all the sampled trees.\n\n## Details\n\n#### Equation Basics\n\nGiven the standard polynomial regression equation: <br/><br/> $E [Y] = \\beta_0 +\\beta_1 B_1(X_1)+\\beta_2 B_2(X_2) + \\cdot \\cdot \\cdot + \\beta_m B_m(X_m)$\n\nAssuming that the ***B~i~*** basis functions in a standard polynomial regression equation are decision trees, the equation becomes: <br/><br/>\n\n$E[Y] = \\phi (\\sum_{j=0}^m g_j(X_i T_j, M_j), \\theta)$<br/> where: <br/> each ***g~j~*** is a tree of the form ***g(X~i~ T~j~, M~j~)*** <br/> ***T~j~*** represents the structure of the binary tree (the set of internal nodes and their decision rules and a set of terminal nodes) <br/> $M_j = \\{\\mu_{1,j}, \\mu_{2,j}, \\cdot\\cdot\\cdot, \\mu_{b,j}\\}$ represents the values at the ***b~j~*** terminal nodes <br/> $\\phi$ represents an arbitrary probability distribution that will be used as the likelihood in the model <br/> $\\theta$ other parameters from $\\phi$ not modeled as a sum of trees <br/>\n\nSome examples of probability distributions might be a Gaussian, a Poisson distribution, or a t-distribution. <br/>\n\n#### Priors\n\nTo specify the BART model, priors need to be specified. These priors help shape the model's structure, control the complexity and guide the learning process. Those prior specifications can be: <br/>\n\n-   Tree Structure and Complexity: Limiting tree depth and size. <br/>\n-   Terminal Node Values: Specifying the distribution for leaf values (i.e. normal distribution) <br/>\n-   Splits and Growth: Controlling the likelihood and nature of tree splits and number of trees within the ensemble. <br/>\n-   Weights and Hyperparameters: Setting priors to balance flexibility and regularization <br/>\n\n**Here we will use:** <br/> **Independence:** In order to simplify the specification of the prior, assume that the structure of the tree ***T~j~*** and the leaf values ***M~j~*** are independent. <br/> **Tree Structure:** <br/> 1 - Probability that a node at depth $d = (0, 1, 2, ...)$ is non terminal is given by $\\alpha^d$ where $\\alpha$ is recommended to be $\\in [0, 0.5)$. <br/> 2 - Distribution over the splitting variable: which covariate is included in the tree ***X~i~*** most commonly uniform over available covariates. <br/> 3 - Distribution over the splitting variable: once the splitting variable is chosen, which value is used to make a decision. see figure below for illustration of splitting rule\n\n![Source: https://bayesiancomputationbook.com/markdown/chp_07.html](images/BinaryTreeWithPartitionSpace.png) Leaf Values $\\mu_{ij}$ and Number of Trees ***m***: Best results are acheived from setting a specific value of m (usually between 10 and 200) and cross validating. <br/>\n\n#### MCMC Sampling Process\n\nMarkov Chain Monte Carlo is used to sample from the posterior distribution of the parameters. It generates samples of the regression trees' structures - arrangement of splits and terminal nodes - exploring possible tree configurations. <br/> Each tree's terminal node values are sampled from their posterior distributions. The node values usually folow a normal distribution with mean and variance specified by the model.<br/>\n\n**Algorithm:** <br/> Start with initial configuration of trees - either randomly generated or based on some specified method. <br/> **Iterate:** **Update Structures.** Changes to the current set of trees are proposed - adding or deleting trees or modifying the structure of existing trees. The changes are accepted or rejected based on probability given data and the prior distribution. **Update Terminal Node Values** For each tree update the values at the terminal nodes. Sample from the posterior distribution of the values given the current tree structure and data. **Acceptance Criteria** Use a set criterion to decide whether to accept proposed changes.<br/> **Convergence:** Continue sampling until the Markov chain converges to the stationary distribution which approximates the posterior distribution of the model parameters.<br/><br/> Once MCMC sampling is complete, predictions for new data are obtained by averaging predictions from all sampled trees. Each tree contributes to the final prediction based on its sampled weight.<br/> The variablity in predictions across sampled trees reflects uncertainty in the model. <br/>\n\n#### Purpose of the Model\n\nThe purpose of the Bayesian Additive Regression Trees with Markov Chain Monte Carlo is to provide a robust and flexible framework for predictive modeling and inference using both the Bayesian principles and regression tree methods. This method models complex relationships and quantifies uncertainty. It prevents overfitting while delivering accurate predictions for a variety of real-world applications. It is the best of the Bayesian inference and tree-based models. \n\n\n## Relation of this topic to course\n\nBART with MCMC is related to the following course topics:<br/>\n\n-   Regression<br/> BART does not assume fixed relationship between predictors and response. Instead it builds flexible models using decision trees capturing non-linear interactions. <br/>\n\n-   Overfitting<br/> Overly complicated tree structures are penalized through the use of priors to prevent overfitting to allow it to generalize well to any new data (test). <br/>\n\n-   Regression Trees<br/> BART represents the sum of multiple regression trees where each tree captures a different feature of the data which allows much more complex datasets to be analyzed. <br/>\n\n-   Bayesian Methods<br/> The prior distribution is applied to the structure of the regression trees influencing how they are built and the complexity of the model. Additionally, posterior sampling like MCMC methods update the structure and their parameters. <br/>\n\n-   Data manipulation<br/> In nearly all cases, data will need to be manipulated. Basic wrangling may need to be done on any data for any model. With BART models, if models do not fit well, feature selection to eliminate noisy features may help the model fit better. <br/>\n\n-   Ability to handle non-linearity and interactions<br/> Because of the use of multiple regression trees that model piecewise functions and then combine to approximate non-linear functions, BART is able to handle more complex relationships. Similarly, the splitting and combining of the decision trees allows for the handling of interactions. <br/>\n\n## Real-world applications\n\nBART with MCMC is useful in a number of different real-life scenarios. The common aspect of those is that the data exhibits non-linear relationships and interactions between the variables, there is need for uncertainty estimates, high accuracy is preferable.<br/><br/>\nSome specific examples include:<br>\n -  **Finance: ** From credit risk assessment to stock price prediction, financial analysis is complex and non-linear and requires in depth analysis which this method can provide.<br/>\n -  **Healthcare: ** Disease risk modeling based on risk factors where there are innumerable interactions to patient outcome predictions are complex and difficult to model with some of the more simple models.<br/>\n -  **Marketing: ** Customer behaviors can be complicated and this model is more able than most to examine the many different aspects.<br/>\n - **Environmental Science: ** From climate modeling to ecological modeling, this method can help scientists examine different variables, their impacts and patterns. This can help with conservation planning and assessing environmental impact. <br/>\n -  **Engineering: ** Detection of anomalies or faults based on sensor data before failure has occurred or in quality control by identifying factors which affect defects. Uncertainty estimates can help specificly identify quality control issues.<br/>\n -  **Epidemiology: ** BART can model everything from disease spread to intervention effectiveness and give uncertainty estimates which are important for understanding and predicting points of possible outbreaks.<br/> \n - **Real Estate: ** BART can be used to predict property values based on location, size, amenities as well as markets. It can also be used to forecast future prices through market trend analysis.<br/>\n\n\n\n\n\n\n## The data\n\n\n\n\n\nThe Boston Housing dataset from the 'MASS' library consists of various features related to housing, including:<br/>\n\n -  crim: Per capita crime rate by town<br/>\n -  zn: Proportion of residential land zoned for lots over 25,000 sq. ft.<br/>\n -  indus: Proportion of non-retail business acres per town<br/>\n -  chas: Charles River dummy variable (1 if tract bounds river; 0 otherwise)<br/>\n -  nox: Nitric oxides concentration (parts per 10 million)<br/>\n -  rm: Average number of rooms per dwelling<br/>\n -  age: Proportion of owner-occupied units built prior to 1940<br/>\n -  dis: Weighted distances to five Boston employment centers<br/>\n -  rad: Index of accessibility to radial highways<br/>\n -  tax: Full-value property tax rate per $10,000<br/>\n -  ptratio: Pupil-teacher ratio by town<br/>\n -  black: 1000(Bk - 0.63)^2 where Bk is the proportion of Black residents by town<br/>\n -  lstat: Percentage of lower status of the population<br/>\n -  medv: Median value of owner-occupied homes in $1000s (target variable)<br/><br/>\n \n\nThe data looks like: \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 6\nColumns: 14\n$ crim    <dbl> 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985\n$ zn      <dbl> 18, 0, 0, 0, 0, 0\n$ indus   <dbl> 2.31, 7.07, 7.07, 2.18, 2.18, 2.18\n$ chas    <int> 0, 0, 0, 0, 0, 0\n$ nox     <dbl> 0.538, 0.469, 0.469, 0.458, 0.458, 0.458\n$ rm      <dbl> 6.575, 6.421, 7.185, 6.998, 7.147, 6.430\n$ age     <dbl> 65.2, 78.9, 61.1, 45.8, 54.2, 58.7\n$ dis     <dbl> 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622\n$ rad     <int> 1, 2, 2, 3, 3, 3\n$ tax     <dbl> 296, 242, 242, 222, 222, 222\n$ ptratio <dbl> 15.3, 17.8, 17.8, 18.7, 18.7, 18.7\n$ black   <dbl> 396.90, 396.90, 392.83, 394.63, 396.90, 394.12\n$ lstat   <dbl> 4.98, 9.14, 4.03, 2.94, 5.33, 5.21\n$ medv    <dbl> 24.0, 21.6, 34.7, 33.4, 36.2, 28.7\n```\n\n\n:::\n:::\n\n\n# Worked Example with given Data\n\nFirst the data is split into a 70% training, 30% test where the target is the median value of owner occupied homes (in thousands of dollars).\n\n::: {.cell}\n\n:::\n\n\nThen, using the bartMachine package, create a BART model. This package is nice because it creates the model and has a lot of the information that is very useful for understanding how well the data fits the model. <br/>\nIt shows the $\\sigma^2$ values where a lower value represents a better fit. <br/>\nL1 and L2 Mean Absolute Error and Mean Squared Error as well as root mean squared error. <br/>\nAdditionally a pseudo R squared value and shapiro-wilk test for normality of the residuals p-value is included as well as a p-value for zero-mean noise (how close to zero the mean remains). <br/>\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nbartMachine initializing with 50 trees...\nbartMachine vars checked...\nbartMachine java init...\nbartMachine factors created...\nbartMachine before preprocess...\nbartMachine after preprocess... 13 total features...\nbartMachine sigsq estimated...\nbartMachine training data finalized...\nNow building bartMachine for regression...\nevaluating in sample data...done\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nbartMachine v1.3.4.1 for regression\n\ntraining data size: n = 367 and p = 13 \nbuilt in 1.2 secs on 1 core, 50 trees, 250 burn-in and 1000 post. samples\n\nsigsq est for y beforehand: 22.204 \navg sigsq estimate after burn-in: 2.90387 \n\nin-sample statistics:\n L1 = 372.1 \n L2 = 610.8 \n rmse = 1.29 \n Pseudo-Rsq = 0.9814\np-val for shapiro-wilk test of normality of residuals: 0.43904 \np-val for zero-mean noise: 0.9972 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Mean Squared Error:  10.3124837547133\"\n```\n\n\n:::\n:::\n\n\nIn addition to the baseline information, it is also of value to look at which variables are of most importance to the model. <br/>\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n.....\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](SupportingInfo_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThere is significant data available through this package including which direction was traversed at each node but here, that information is somewhat beyond the pale. <br/>\nWhat is more useful, is examining the relationship between the predicted values and the actual values. <br/>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](SupportingInfo_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nThe posterior distribution of predictions can be used to provide a probabilistic description of possible outcomes that reflect both uncertainty and variability. The average provides a central tendency estimate of the prediction. <br/>\n\n::: {.cell}\n::: {.cell-output-display}\n![](SupportingInfo_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThe covariance importance test can show whether the combined features have significant impact on outcome (p-value <0.05) <br/>\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nbartMachine omnibus test for covariate importance\n..................................................\n..................................................\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](SupportingInfo_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\np_val =  0 \n```\n\n\n:::\n:::\n\n\nThe proportions of each variable can be shown as a list to get a more precise list so that, if desired or needed, lower value variables could be removed to improved the model. <br/>\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n      crim         zn      indus       chas        nox         rm        age \n0.09152734 0.04728021 0.08008774 0.07489269 0.12249824 0.11089072 0.04697585 \n       dis        rad        tax    ptratio      black      lstat \n0.08224972 0.04498179 0.06513229 0.05522496 0.05261169 0.12564676 \n```\n\n\n:::\n:::\n\n\nLooking at the Markov-Chain Monte Carlo Uncertainty can give a good visual representation of the uncertainty at different points throughout the data. \n\n::: {.cell}\n::: {.cell-output-display}\n![](SupportingInfo_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n# Conclusions, Recommendations, Future Work\n\nBART with MCMC is one of the better methods for modeling datasets with non-linear relationships or complicated interactions without requiring explicit specifications for all interactions and relationships. The inclusion of uncertainty data increases its value by allowing users to have an implicit understanding of how much to trust it. It is useful in many different real-world applications across a wide variety of fields. As data collection tools improve, datasets will become more complicated and this model will continue to have value because of its ability to deal with large complicated datasets.\n\n# References\n\nBayesian Modeling and Computation in Python. (2022). Bayesiancomputationbook.com. https://bayesiancomputationbook.com , Hugh A. Chipman, Edward I. George, and Robert E. McCulloch. <br/>\n\nVeronika Ročková and Enakshi Saha. On theory for bart. In The 22nd International Conference on Artificial Intelligence and Statistics, 2839–2848. PMLR, 2019. <br/>\n",
    "supporting": [
      "SupportingInfo_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}