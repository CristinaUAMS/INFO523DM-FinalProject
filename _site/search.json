[
  {
    "objectID": "SupportingInfo.html",
    "href": "SupportingInfo.html",
    "title": "From Trees to Chains: Implementing Bayesian Additive Regression with MCMC",
    "section": "",
    "text": "Bayesian Additive Regression Trees (BART) with Markov Chain Monte Carlo (MCMC) is a powerful approach for predictive modeling. This paper and presentation explore BART algorithm focusing on the additive tree structure and the role of MCMC in efficiently estimating model parameters. Contained within is a worked example using a publicly available dataset of earned tips demonstrating how BART with MCMC captures the complex relationships and provides uncertainty estimates. Further, there is discussion of other real-world applications of this approach including (but not limited to): healthcare risk prediction, environmental monitoring, and market forecasting."
  },
  {
    "objectID": "SupportingInfo.html#background",
    "href": "SupportingInfo.html#background",
    "title": "From Trees to Chains: Implementing Bayesian Additive Regression with MCMC",
    "section": "Background",
    "text": "Background\nBayesian Additive Regression Trees with Markov Chain Monte Carlo creates a robust approach for predictive modeling. It can model complex, non-linear relationships without specific modeling of interactions. Bayesian approach provides uncertainty estimates which help give context and understanding to the model, especially with respect to reliability. By averaging over many trees, it is less likely to be over-fitted than if a single, large tree were used. BART is a non-parametric Bayesian method for regression and classification. It can model complex relationships by combining multiple simple decision trees to predict an outcome. The additive refers to the fact that the overall prediction is the sum of the contributions from each tree. It uses Bayesian inference to estimate the posterior distribution of the model’s parameters providing point estimates and measures of uncertainty in predictions. MCMC methods generate samples from the posterior distribution of the parameters by constructing a Markov chain with the desired distribution as the equilibrium distribution. Using the combined method, the overly complex trees are penalized to prevent over-fitting. During sampling, the trees and their structure are updated based on data and prior distribution. Once sampling is complete, the final prediction for a new observation is developed by averaging the predictions from all the sampled trees."
  },
  {
    "objectID": "SupportingInfo.html#details",
    "href": "SupportingInfo.html#details",
    "title": "From Trees to Chains: Implementing Bayesian Additive Regression with MCMC",
    "section": "Details",
    "text": "Details\n\nEquation Basics\nGiven the standard polynomial regression equation:  \\(E [Y] = \\beta_0 +\\beta_1 B_1(X_1)+\\beta_2 B_2(X_2) + \\cdot \\cdot \\cdot + \\beta_m B_m(X_m)\\)\nAssuming that the Bi basis functions in a standard polynomial regression equation are decision trees, the equation becomes: \n\\(E[Y] = \\phi (\\sum_{j=0}^m g_j(X_i T_j, M_j), \\theta)\\) where:  each gj is a tree of the form g(Xi Tj, Mj)  Tj represents the structure of the binary tree (the set of internal nodes and their decision rules and a set of terminal nodes)  \\(M_j = \\{\\mu_{1,j}, \\mu_{2,j}, \\cdot\\cdot\\cdot, \\mu_{b,j}\\}\\) represents the values at the bj terminal nodes  \\(\\phi\\) represents an arbitrary probability distribution that will be used as the likelihood in the model  \\(\\theta\\) other parameters from \\(\\phi\\) not modeled as a sum of trees \nSome examples of probability distributions might be a Gaussian, a Poisson distribution, or a t-distribution. \n\n\nPriors\nTo specify the BART model, priors need to be specified. These priors help shape the model’s structure, control the complexity and guide the learning process. Those prior specifications can be: \n\nTree Structure and Complexity: Limiting tree depth and size. \nTerminal Node Values: Specifying the distribution for leaf values (i.e. normal distribution) \nSplits and Growth: Controlling the likelihood and nature of tree splits and number of trees within the ensemble. \nWeights and Hyperparameters: Setting priors to balance flexibility and regularization \n\nHere we will use:  Independence: In order to simplify the specification of the prior, assume that the structure of the tree Tj and the leaf values Mj are independent.  Tree Structure:  1 - Probability that a node at depth \\(d = (0, 1, 2, ...)\\) is non terminal is given by \\(\\alpha^d\\) where \\(\\alpha\\) is recommended to be \\(\\in [0, 0.5)\\).  2 - Distribution over the splitting variable: which covariate is included in the tree Xi most commonly uniform over available covariates.  3 - Distribution over the splitting variable: once the splitting variable is chosen, which value is used to make a decision. see figure below for illustration of splitting rule\n Leaf Values \\(\\mu_{ij}\\) and Number of Trees m: Best results are acheived from setting a specific value of m (usually between 10 and 200) and cross validating. \n\n\nMCMC Sampling Process\nMarkov Chain Monte Carlo is used to sample from the posterior distribution of the parameters. It generates samples of the regression trees’ structures - arrangement of splits and terminal nodes - exploring possible tree configurations.  Each tree’s terminal node values are sampled from their posterior distributions. The node values usually folow a normal distribution with mean and variance specified by the model.\nAlgorithm:  Start with initial configuration of trees - either randomly generated or based on some specified method.  Iterate: Update Structures. Changes to the current set of trees are proposed - adding or deleting trees or modifying the structure of existing trees. The changes are accepted or rejected based on probability given data and the prior distribution. Update Terminal Node Values For each tree update the values at the terminal nodes. Sample from the posterior distribution of the values given the current tree structure and data. Acceptance Criteria Use a set criterion to decide whether to accept proposed changes. Convergence: Continue sampling until the Markov chain converges to the stationary distribution which approximates the posterior distribution of the model parameters. Once MCMC sampling is complete, predictions for new data are obtained by averaging predictions from all sampled trees. Each tree contributes to the final prediction based on its sampled weight. The variablity in predictions across sampled trees reflects uncertainty in the model. \n\n\nPurpose of the Model\nThe purpose of the Bayesian Additive Regression Trees with Markov Chain Monte Carlo is to provide a robust and flexible framework for predictive modeling and inference using both the Bayesian principles and regression tree methods. This method models complex relationships and quantifies uncertainty. It prevents overfitting while delivering accurate predictions for a variety of real-world applications. It is the best of the Bayesian inference and tree-based models."
  },
  {
    "objectID": "SupportingInfo.html#relation-of-this-topic-to-course",
    "href": "SupportingInfo.html#relation-of-this-topic-to-course",
    "title": "From Trees to Chains: Implementing Bayesian Additive Regression with MCMC",
    "section": "Relation of this topic to course",
    "text": "Relation of this topic to course\nBART with MCMC is related to the following course topics:\n\nRegression\nOverfitting\nRegression Trees\nBayesian Methods\nData manipulation\nAbility to handle non-linearity and interactions"
  },
  {
    "objectID": "SupportingInfo.html#real-world-applications",
    "href": "SupportingInfo.html#real-world-applications",
    "title": "From Trees to Chains: Implementing Bayesian Additive Regression with MCMC",
    "section": "Real-world applications",
    "text": "Real-world applications"
  },
  {
    "objectID": "SupportingInfo.html#the-data",
    "href": "SupportingInfo.html#the-data",
    "title": "From Trees to Chains: Implementing Bayesian Additive Regression with MCMC",
    "section": "The data",
    "text": "The data\nThis data is the tip data collected by a waiter over a period of a couple months. The variables include:  * tip in dollars * bill in dollars * sex of the bill payer * whether there were smokers in the party * day of the week * time of day * size of the party Data found at https://rdrr.io/cran/reshape2/man/tips.html  Reference given: Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing"
  }
]