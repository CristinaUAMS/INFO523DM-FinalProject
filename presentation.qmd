---
title: "From Trees to Chains: Implementing Bayesian Additive Regression with MCMC"
subtitle: "INFO 523 - Summer 2024 - Final Project"
description: "Presentation on Bayesian Additive Regression Trees with MCMC"
author: 
- name: Cristina Lafuente
  affiliation: Department of Electrical and Computer Engineering, University of Arizona
title-slide-attributes:
  data-background-image: images/three-dimensional-tree-with-foliage.jpg
  data-background-size: stretch
  data-background-opacity: "0.4"
  data-slide-number: none
format:
  revealjs:
    theme:  ['css/customtheming.scss']
  
editor: visual
execute:
  echo: false
---

```{r}
#| label: load-packages
#| include: false

options(java.parameters = "-Xmx5g")
if (!require("pacman")) 
  install.packages("pacman")

pacman::p_load(bartMachine,
               bartMan,
               caTools,
               dplyr,
               ggplot2,
               ggthemes,
               here,
               MASS,
               partykit,
               pdp,
               rJava,
               rpart,
               rpart.plot,
               tidyverse,
               treemap)


knitr::opts_chunk$set(
  fig.retina = 3, 
  dpi = 300, 
  fig.width = 10, 
  fig.asp = 0.618 
  )
```

```{r}
#| label: load-data
#| include: false
data("Boston")
boston_data <- Boston

```

## Bayesian Additive Regression Trees

![Image source: Catalyzex](images/BARTimage.png)

<br/> Multiple regression trees put together in a larger ensemble to model complex data with many features.

## Basic Background and Purpose
::: {.r-fit-text}

 -  Bayesian Additive Regression Trees with Markov Chain Monte Carlo can model complex, non-linear relationships without explicit modeling of interactions. <br/>
 -  Bayesian approach provides uncertainty estimates giving context and understanding to reliability of the model. <br/>
 -  By averaging over many trees, it is less likely to be over-fitted than if a single, large tree were used. <br/>
 -  They are additive because the overall prediction is the average of the predictions of the contributions from each tree. <br/>
 -  MCMC methods generate samples from the posterior distribution of the parameters by constructing a Markov chain with the desired distribution as the equilibrium distribution.<br/>

:::

## Details
::: {.r-fit-text}
EQUATION: <br/>
Given the standard polynomial regression equation: <br/><br/> $E [Y] = \beta_0 +\beta_1 B_1(X_1)+\beta_2 B_2(X_2) + \cdot \cdot \cdot + \beta_m B_m(X_m)$

Assuming that the ***B~i~*** basis functions in a standard polynomial regression equation are decision trees, the equation becomes: <br/>

$E[Y] = \phi (\sum_{j=0}^m g_j(X_i T_j, M_j), \theta)$<br/> where: <br/> each ***g~j~*** is a tree of the form ***g(X~i~ T~j~, M~j~)*** <br/> ***T~j~*** represents the structure of the binary tree (the set of internal nodes and their decision rules and a set of terminal nodes) <br/> $M_j = \{\mu_{1,j}, \mu_{2,j}, \cdot\cdot\cdot, \mu_{b,j}\}$ represents the values at the ***b~j~*** terminal nodes <br/> $\phi$ represents an arbitrary probability distribution that will be used as the likelihood in the model <br/> $\theta$ other parameters from $\phi$ not modeled as a sum of trees <br/>

Some examples of probability distributions might be a Gaussian, a Poisson distribution, or a t-distribution. <br/>

:::
---

::: {.r-fit-text}
PRIORS <br/>

To specify the BART model, priors need to be specified. These priors help shape the model's structure, control the complexity and guide the learning process. Those prior specifications can be: <br/>

-   Tree Structure and Complexity: Limiting tree depth and size. <br/>
-   Terminal Node Values: Specifying the distribution for leaf values (i.e. normal distribution) <br/>
-   Splits and Growth: Controlling the likelihood and nature of tree splits and number of trees within the ensemble. <br/>
-   Weights and Hyperparameters: Setting priors to balance flexibility and regularization <br/>

MCMC UNCERTAINTY 
Markov Chain Monte Carlo is used to sample from the posterior distribution of the parameters. It generates samples of the regression trees' structures - arrangement of splits and terminal nodes - exploring possible tree configurations. <br/> Each tree's terminal node values are sampled from their posterior distributions. The node values usually follow a normal distribution with mean and variance specified by the model.<br/><br/>
 Once MCMC sampling is complete, predictions for new data are obtained by averaging predictions from all sampled trees. Each tree contributes to the final prediction based on its sampled weight.<br/> The variablity in predictions across sampled trees reflects uncertainty in the model. <br/>
:::

## Purpose of the Model

The purpose of the Bayesian Additive Regression Trees with Markov Chain Monte Carlo is to provide a robust and flexible framework for predictive modeling and inference using both the Bayesian principles and regression tree methods. This method models complex relationships and quantifies uncertainty. It prevents overfitting while delivering accurate predictions for a variety of real-world applications. It offers the best of the Bayesian inference and tree-based models. 

## Relation of this topic to course
::: {.r-fit-text}
BART with MCMC is related to the following course topics:<br/>

-   Regression<br/> BART builds flexible models using decision trees capturing non-linear interactions. <br/>

-   Overfitting<br/> Overly complicated tree structures are penalized through the use of priors to prevent overfitting to allow it to generalize well to any new data (test). <br/>

-   Regression Trees<br/> BART represents the sum of multiple regression trees where each tree captures a different feature of the data. <br/>

-   Bayesian Methods<br/> The prior distribution is applied to the structure of the regression trees influencing how they are built and the complexity of the model. Additionally, posterior sampling like MCMC methods update the structure and their parameters. <br/>

-   Data manipulation<br/> With BART models, if models do not fit well, feature selection to eliminate noisy features may help the model fit better. <br/>

-   Ability to handle non-linearity and interactions<br/> BART is able to handle more complex relationships and the splitting and combining of the decision trees allows for the handling of interactions. <br/>

:::

## Real-world applications
::: {.r-fit-text}
BART with MCMC is useful in real-world applications where data exhibits non-linear relationships and there are interactions between the variables, there is need for uncertainty estimates, especially if high accuracy is preferable.<br/>
Some specific examples include:<br>
 -  **Finance: ** From credit risk assessment to stock price prediction, financial analysis is complex and non-linear and requires in depth analysis.<br/>
 -  **Healthcare: ** Disease risk modeling based on risk factors where there are innumerable interactions to patient outcome predictions are complex.<br/>
 -  **Marketing: ** Customer behaviors can be complicated and this model is more able than most to examine the many different aspects.<br/>
 - **Environmental Science: ** From climate modeling to ecological modeling, this method can help scientists examine different variables, their impacts and patterns. This can help with conservation planning and assessing environmental impact. <br/>
 -  **Engineering: ** Detection of anomalies or faults based on sensor data before failure has occurred or in quality control by identifying factors which affect defects. Uncertainty estimates can help specifically identify quality control issues.<br/>
 -  **Epidemiology: ** BART can model everything from disease spread to intervention effectiveness and give uncertainty estimates which are important for understanding and predicting points of possible outbreaks.<br/> 
 - **Real Estate: ** BART can be used to predict property values based on location, size, amenities as well as markets. It can also be used to forecast future prices through market trend analysis.<br/>
 
:::

## The data
::: {.r-fit-text}
The Boston Housing dataset from the 'MASS' library consists of various features related to housing, including:<br/>

 -  crim: Per capita crime rate by town<br/>
 -  zn: Proportion of residential land zoned for lots over 25,000 sq. ft.<br/>
 -  indus: Proportion of non-retail business acres per town<br/>
 -  chas: Charles River dummy variable (1 if tract bounds river; 0 otherwise)<br/>
 -  nox: Nitric oxides concentration (parts per 10 million)<br/>
 -  rm: Average number of rooms per dwelling<br/>
 -  age: Proportion of owner-occupied units built prior to 1940<br/>
 -  dis: Weighted distances to five Boston employment centers<br/>
 -  rad: Index of accessibility to radial highways<br/>
 -  tax: Full-value property tax rate per $10,000<br/>
 -  ptratio: Pupil-teacher ratio by town<br/>
 -  black: 1000(Bk - 0.63)^2 where Bk is the proportion of Black residents by town<br/>
 -  lstat: Percentage of lower status of the population<br/>
 -  medv: Median value of owner-occupied homes in $1000s (target variable)<br/><br/>

::: 
---
The data looks like: 

```{r}
#| label: data
#| 


glimpse(head(boston_data))
```
## Worked Example with given Data
::: {.r-fit-text}
First the data is split into a 70% training, 30% test where the target is the median value of owner occupied homes (in thousands of dollars).
```{r}
#|label: splitting-data

set.seed(1123)

## Split data into training and testing 
split <- sample.split(boston_data$medv, SplitRatio = 0.7)
train_data <- subset(boston_data, split == TRUE)
test_data <- subset(boston_data, split == FALSE)

X_train <- train_data[, -14]
y_train <- train_data$medv

X_test <- test_data[, -14]
y_test <- test_data$medv

```

Then, using the bartMachine package, create a BART model. This package is nice because it creates the model and has a lot of the information that is very useful for understanding how well the data fits the model. <br/>
It shows the $\sigma^2$ values where a lower value represents a better fit. <br/>
L1 and L2 Mean Absolute Error and Mean Squared Error as well as root mean squared error. <br/>
Additionally a pseudo R squared value and shapiro-wilk test for normality of the residuals p-value is included as well as a p-value for zero-mean noise (how close to zero the mean remains). <br/>
```{r}
#|label: bart
bart_model <- bartMachine(X_train, y_train)
y_pred <- predict(bart_model, X_test)
bart_model
mse <- mean((y_test - y_pred)^2)
print(paste('Mean Squared Error: ', mse))
```

:::
---
```{r}
bart_model
print(paste('Mean Squared Error: ', mse))
```
---
In addition to the baseline information, it is also of value to look at which variables are of most importance to the model. <br/>
```{r}
#|label: analyze

importance <- investigate_var_importance(bart_model)
```
---
The relationship between the predicted values and the actual values. <br/>

```{r}
#|label: plotting data
#node_data <- extract_raw_node_data(bart_model)

prediction <- predict(bart_model, X_train)
ggplot(data.frame(Actual = y_train, Predicted = prediction),
       aes(x = Actual, y = Predicted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  xlab("Actual Values") +
  ylab("Predicted Values") +
  ggtitle("Actual V Predicted Values")+
  theme(
    text = element_text(color = "white"),
    axis.text = element_text(color = "white"),
    plot.background = element_rect(fill = "steelblue4"),
    panel.background = element_rect(fill = "cornsilk", color = "grey")

  )
```
---
The posterior distribution of predictions can be used to provide a probabilistic description of possible outcomes that reflect both uncertainty and variability. The average provides a central tendency estimate of the prediction. <br/>
```{r}
#|label: more-plotting
posterior <- bart_machine_get_posterior(bart_model, X_train)


hist(posterior$y_hat_posterior_samples,
     main = "Posterior Distribution of Predictions",
     xlab = "Value", col = "darkslateblue")

error_assumptions <- check_bart_error_assumptions(bart_model,
                                                  hetero_plot = "yhats")
```
## Covariance Importance
The covariance importance test can show whether the combined features have significant impact on outcome (p-value <0.05) <br/>

```{r}
#|label: more-plotting2


cov_importance_test(bart_model)

```
## Variable Proportions
The proportions of each variable can be shown as a list to get a more precise list so that, if desired or needed, lower value variables could be removed to improved the model. <br/>
```{r}

#|label: variable_proportions


prop_incl <- get_var_props_over_chain(bart_model)
print(prop_incl)
```
## MCMC Uncertainty
Looking at the Markov-Chain Monte Carlo Uncertainty can give a good visual representation of the uncertainty at different points throughout the data.
```{r}
#|label: more-plotting3
posterior_samples <- posterior$y_hat_posterior_samples

predictions <- as.matrix(predict(bart_model, X_train))

predicted_means <- rowMeans(posterior_samples)
predicted_lows <- apply(posterior_samples, 1,
                        function(x) quantile(x, probs = 0.025))
predicted_highs <- apply(posterior_samples, 1,
                         function(x) quantile(x, probs = 0.975))

prediction_intervals <- data.frame(
  Mean = predicted_means,
  Lower = predicted_lows,
  Upper = predicted_highs
)

plot_data <- data.frame(
  observed = y_train,
  predicted = predicted_means,
  lower = predicted_lows,
  upper = predicted_highs
)


ggplot(plot_data, aes(x = predicted, y = observed)) +

  geom_errorbar(aes(ymin = lower, ymax = upper),
                width = .1, color = "lightslateblue") +
  geom_point(color = "royalblue4", size = 1) +
  geom_abline(intercept = 0, slope = 1, color = "magenta3") +
  scale_y_continuous() +
  labs(
    title = "Predictions with Uncertainty Intervals",
    x = "Predicted",
    y = "Observed ") +
  theme(
    text = element_text(color = "white"),
    axis.text = element_text(color = "white"),
    plot.background = element_rect(fill = "steelblue4"),
    panel.background = element_rect(fill = "cornsilk", color = "grey")

  )
```
## Conclusions, Recommendations, Future Work

BART with MCMC is one of the better methods for modeling datasets with non-linear relationships or complicated interactions without requiring explicit specifications for all interactions and relationships. The inclusion of uncertainty data increases its value by allowing users to have an implicit understanding of how much to trust it. It is useful in many different real-world applications across a wide variety of fields. As data collection tools improve, datasets will become more complicated and this model will continue to have value because of its ability to deal with large complicated datasets.

## References

Bayesian Modeling and Computation in Python. (2022). Bayesiancomputationbook.com. https://bayesiancomputationbook.com , Hugh A. Chipman, Edward I. George, and Robert E. McCulloch. <br/>

CatalyzeX. (2023). Sharded Bayesian Additive Regression Trees. CatalyzeX. https://www.catalyzex.com/paper/sharded-bayesian-additive-regression-trees <br/>

Veronika Ročková and Enakshi Saha. On theory for bart. In The 22nd International Conference on Artificial Intelligence and Statistics, 2839–2848. PMLR, 2019. <br/>
